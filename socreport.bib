@ARTICLE{10.3389/fnbot.2019.00018,
  AUTHOR          = {Bing, Zhenshan and Baumann, Ivan and Jiang, Zhuangyi and
                  Huang, Kai and Cai, Caixia and Knoll, Alois},
  TITLE           = {Supervised Learning in SNN via Reward-Modulated
                  Spike-Timing-Dependent Plasticity for a Target Reaching
                  Vehicle},
  JOURNAL         = {Frontiers in Neurorobotics},
  VOLUME          = 13,
  PAGES           = 18,
  YEAR            = 2019,
  URL             =
                  {https://www.frontiersin.org/article/10.3389/fnbot.2019.00018},
  DOI             = {10.3389/fnbot.2019.00018},
  ISSN            = {1662-5218},
  ABSTRACT        = {Spiking neural networks (SNNs) offer many advantages over
                  traditional artificial neural networks (ANNs) such as
                  biological plausibility, fast information processing, and
                  energy efficiency. Although SNNs have been used to solve a
                  variety of control tasks using the Spike-Timing-Dependent
                  Plasticity (STDP) learning rule, existing solutions usually
                  involve hard-coded network architectures solving specific
                  tasks rather than solving different kinds of tasks generally.
                  This results in neglecting one of the biggest advantages of
                  ANNs, i.e., being general-purpose and easy-to-use due to their
                  simple network architecture, which usually consists of an
                  input layer, one or multiple hidden layers and an output
                  layer. This paper addresses the problem by introducing an
                  end-to-end learning approach of spiking neural networks
                  constructed with one hidden layer and reward-modulated
                  Spike-Timing-Dependent Plasticity (R-STDP) synapses in an
                  all-to-all fashion. We use the supervised reward-modulated
                  Spike-Timing-Dependent-Plasticity learning rule to train two
                  different SNN-based sub-controllers to replicate a desired
                  obstacle avoiding and goal approaching behavior, provided by
                  pre-generated datasets. Together they make up a
                  target-reaching controller, which is used to control a
                  simulated mobile robot to reach a target area while avoiding
                  obstacles in its path. We demonstrate the performance and
                  effectiveness of our trained SNNs to achieve target reaching
                  tasks in different unknown scenarios.}
}

@ARTICLE{10.3389/fncom.2017.00024,
  author          = {Scellier, Benjamin and Bengio, Yoshua},
  title           = {Equilibrium Propagation: Bridging the Gap between
                  Energy-Based Models and Backpropagation},
  journal         = {Frontiers in Computational Neuroscience},
  volume          = 11,
  pages           = 24,
  year            = 2017,
  url             =
                  {https://www.frontiersin.org/article/10.3389/fncom.2017.00024},
  doi             = {10.3389/fncom.2017.00024},
  issn            = {1662-5188},
  abstract        = {We introduce Equilibrium Propagation, a learning framework
                  for energy-based models. It involves only one kind of neural
                  computation, performed in both the first phase (when the
                  prediction is made) and the second phase of training (after
                  the target or prediction error is revealed). Although this
                  algorithm computes the gradient of an objective function just
                  like Backpropagation, it does not need a special computation
                  or circuit for the second phase, where errors are implicitly
                  propagated. Equilibrium Propagation shares similarities with
                  Contrastive Hebbian Learning and Contrastive Divergence while
                  solving the theoretical issues of both algorithms: our
                  algorithm computes the gradient of a well-defined objective
                  function. Because the objective function is defined in terms
                  of local perturbations, the second phase of Equilibrium
                  Propagation corresponds to only nudging the prediction (fixed
                  point or stationary distribution) toward a configuration that
                  reduces prediction error. In the case of a recurrent
                  multi-layer supervised network, the output units are slightly
                  nudged toward their target in the second phase, and the
                  perturbation introduced at the output layer propagates
                  backward in the hidden layers. We show that the signal
                  “back-propagated” during this second phase corresponds to the
                  propagation of error derivatives and encodes the gradient of
                  the objective function, when the synaptic update corresponds
                  to a standard form of spike-timing dependent plasticity. This
                  work makes it more plausible that a mechanism similar to
                  Backpropagation could be implemented by brains, since leaky
                  integrator neural computation performs both inference and
                  error back-propagation in our model. The only local difference
                  between the two phases is whether synaptic changes are allowed
                  or not. We also show experimentally that multi-layer
                  recurrently connected networks with 1, 2, and 3 hidden layers
                  can be trained by Equilibrium Propagation on the
                  permutation-invariant MNIST task.}
}

@ARTICLE{10.3389/fninf.2018.00089,
  AUTHOR          = {Hazan, Hananel and Saunders, Daniel J. and Khan, Hassaan
                  and Patel, Devdhar and Sanghavi, Darpan T. and Siegelmann,
                  Hava T. and Kozma, Robert},
  TITLE           = {BindsNET: A Machine Learning-Oriented Spiking Neural
                  Networks Library in Python},
  JOURNAL         = {Frontiers in Neuroinformatics},
  VOLUME          = 12,
  PAGES           = 89,
  YEAR            = 2018,
  URL             =
                  {https://www.frontiersin.org/article/10.3389/fninf.2018.00089},
  DOI             = {10.3389/fninf.2018.00089},
  ISSN            = {1662-5196},
}

@ARTICLE{10.3389/fnins.2015.00481,
  AUTHOR          = {Serrano-Gotarredona, Teresa and Linares-Barranco, Bernabé},
  TITLE           = {Poker-DVS and MNIST-DVS. Their History, How They Were Made,
                  and Other Details},
  JOURNAL         = {Frontiers in Neuroscience},
  VOLUME          = 9,
  PAGES           = 481,
  YEAR            = 2015,
  URL             =
                  {https://www.frontiersin.org/article/10.3389/fnins.2015.00481},
  DOI             = {10.3389/fnins.2015.00481},
  ISSN            = {1662-453X},
  ABSTRACT        = {This article reports on two databases for event-driven
                  object recognition using a Dynamic Vision Sensor (DVS). The
                  first, which we call Poker-DVS and is being released together
                  with this article, was obtained by browsing specially made
                  poker card decks in front of a DVS camera for 2–4 s. Each card
                  appeared on the screen for about 20–30 ms. The poker pips were
                  tracked and isolated off-line to constitute the 131-recording
                  Poker-DVS database. The second database, which we call
                  MNIST-DVS and which was released in December 2013, consists of
                  a set of 30,000 DVS camera recordings obtained by displaying
                  10,000 moving symbols from the standard MNIST 70,000-picture
                  database on an LCD monitor for about 2–3 s each. Each of the
                  10,000 symbols was displayed at three different scales, so
                  that event-driven object recognition algorithms could easily
                  be tested for different object sizes. This article tells the
                  story behind both databases, covering, among other aspects,
                  details of how they work and the reasons for their creation.
                  We provide not only the databases with corresponding scripts,
                  but also the scripts and data used to generate the figures
                  shown in this article (as Supplementary Material).}
}

@ARTICLE{10.3389/fnins.2016.00508,
  AUTHOR          = {Lee, Jun Haeng and Delbruck, Tobi and Pfeiffer, Michael},
  TITLE           = {Training Deep Spiking Neural Networks Using
                  Backpropagation},
  JOURNAL         = {Frontiers in Neuroscience},
  VOLUME          = 10,
  PAGES           = 508,
  YEAR            = 2016,
  URL             =
                  {https://www.frontiersin.org/article/10.3389/fnins.2016.00508},
  DOI             = {10.3389/fnins.2016.00508},
  ISSN            = {1662-453X},
  ABSTRACT        = {Deep spiking neural networks (SNNs) hold the potential for
                  improving the latency and energy efficiency of deep neural
                  networks through data-driven event-based computation. However,
                  training such networks is difficult due to the
                  non-differentiable nature of spike events. In this paper, we
                  introduce a novel technique, which treats the membrane
                  potentials of spiking neurons as differentiable signals, where
                  discontinuities at spike times are considered as noise. This
                  enables an error backpropagation mechanism for deep SNNs that
                  follows the same principles as in conventional deep networks,
                  but works directly on spike signals and membrane potentials.
                  Compared with previous methods relying on indirect training
                  and conversion, our technique has the potential to capture the
                  statistics of spikes more precisely. We evaluate the proposed
                  framework on artificially generated events from the original
                  MNIST handwritten digit benchmark, and also on the N-MNIST
                  benchmark recorded with an event-based dynamic vision sensor,
                  in which the proposed method reduces the error rate by a
                  factor of more than three compared to the best previous SNN,
                  and also achieves a higher accuracy than a conventional
                  convolutional neural network (CNN) trained and tested on the
                  same data. We demonstrate in the context of the MNIST task
                  that thanks to their event-driven operation, deep SNNs (both
                  fully connected and convolutional) trained with our method
                  achieve accuracy equivalent with conventional neural networks.
                  In the N-MNIST example, equivalent accuracy is achieved with
                  about five times fewer computational operations.}
}

@ARTICLE{10.3389/fnins.2018.00435,
  AUTHOR          = {Lee, Chankyu and Panda, Priyadarshini and Srinivasan,
                  Gopalakrishnan and Roy, Kaushik},
  TITLE           = {Training Deep Spiking Convolutional Neural Networks With
                  STDP-Based Unsupervised Pre-training Followed by Supervised
                  Fine-Tuning},
  JOURNAL         = {Frontiers in Neuroscience},
  VOLUME          = 12,
  PAGES           = 435,
  YEAR            = 2018,
  URL             =
                  {https://www.frontiersin.org/article/10.3389/fnins.2018.00435},
  DOI             = {10.3389/fnins.2018.00435},
  ISSN            = {1662-453X},
  ABSTRACT        = {Spiking Neural Networks (SNNs) are fast becoming a
                  promising candidate for brain-inspired neuromorphic computing
                  because of their inherent power efficiency and impressive
                  inference accuracy across several cognitive tasks such as
                  image classification and speech recognition. The recent
                  efforts in SNNs have been focused on implementing deeper
                  networks with multiple hidden layers to incorporate
                  exponentially more difficult functional representations. In
                  this paper, we propose a pre-training scheme using
                  biologically plausible unsupervised learning, namely
                  Spike-Timing-Dependent-Plasticity (STDP), in order to better
                  initialize the parameters in multi-layer systems prior to
                  supervised optimization. The multi-layer SNN is comprised of
                  alternating convolutional and pooling layers followed by
                  fully-connected layers, which are populated with leaky
                  integrate-and-fire spiking neurons. We train the deep SNNs in
                  two phases wherein, first, convolutional kernels are
                  pre-trained in a layer-wise manner with unsupervised learning
                  followed by fine-tuning the synaptic weights with spike-based
                  supervised gradient descent backpropagation. Our experiments
                  on digit recognition demonstrate that the STDP-based
                  pre-training with gradient-based optimization provides
                  improved robustness, faster (~2.5 ×) training time and better
                  generalization compared with purely gradient-based training
                  without pre-training.}
}

@Article{Backus_1978,
  author          = {Backus, John},
  title           = {Can programming be liberated from the von Neumann style?: a
                  functional style and its algebra of programs},
  year            = 1978,
  volume          = 21,
  number          = 8,
  month           = {Aug},
  pages           = {613–641},
  issn            = {0001-0782},
  doi             = {10.1145/359576.359579},
  url             = {http://dx.doi.org/10.1145/359576.359579},
  journal         = {Communications of the ACM},
  publisher       = {Association for Computing Machinery (ACM)}
}

@Article{Cybenko1989,
  author          = "Cybenko, G.",
  title           = "Approximation by superpositions of a sigmoidal function",
  journal         = "Mathematics of Control, Signals and Systems",
  year            = 1989,
  month           = "Dec",
  day             = 01,
  volume          = 2,
  number          = 4,
  pages           = "303--314",
  abstract        = "In this paper we demonstrate that finite linear
                  combinations of compositions of a fixed, univariate function
                  and a set of affine functionals can uniformly approximate any
                  continuous function ofn real variables with support in the
                  unit hypercube; only mild conditions are imposed on the
                  univariate function. Our results settle an open question about
                  representability in the class of single hidden layer neural
                  networks. In particular, we show that arbitrary decision
                  regions can be arbitrarily well approximated by continuous
                  feedforward neural networks with only a single internal,
                  hidden layer and any continuous sigmoidal nonlinearity. The
                  paper discusses approximation properties of other possible
                  types of nonlinearities that might be implemented by
                  artificial neural networks.",
  issn            = "1435-568X",
  doi             = "10.1007/BF02551274",
  url             = "https://doi.org/10.1007/BF02551274"
}

@ARTICLE{Lacey:2015,
  AUTHOR          = {Lacey, S. and Sathian, K. },
  TITLE           = {{C}rossmodal and multisensory interactions between vision
                  and touch},
  YEAR            = 2015,
  JOURNAL         = {Scholarpedia},
  VOLUME          = 10,
  NUMBER          = 3,
  PAGES           = 7957,
  DOI             = {10.4249/scholarpedia.7957},
  NOTE            = {revision \#150498}
}

@article{MAASS19971659,
  title           = "Networks of spiking neurons: The third generation of neural
                  network models",
  journal         = "Neural Networks",
  volume          = 10,
  number          = 9,
  pages           = "1659 - 1671",
  year            = 1997,
  issn            = "0893-6080",
  doi             = "https://doi.org/10.1016/S0893-6080(97)00011-7",
  url             =
                  "http://www.sciencedirect.com/science/article/pii/S0893608097000117",
  author          = "Wolfgang Maass",
  keywords        = "Spiking neuron, Integrate-and-fire neutron, Computational
                  complexity, Sigmoidal neural nets, Lower bounds",
  abstract        = "The computational power of formal models for networks of
                  spiking neurons is compared with that of other neural network
                  models based on McCulloch Pitts neurons (i.e., threshold
                  gates), respectively, sigmoidal gates. In particular it is
                  shown that networks of spiking neurons are, with regard to the
                  number of neurons that are needed, computationally more
                  powerful than these other neural network models. A concrete
                  biologically relevant function is exhibited which can be
                  computed by a single spiking neuron (for biologically
                  reasonable values of its parameters), but which requires
                  hundreds of hidden units on a sigmoidal neural net. On the
                  other hand, it is known that any function that can be computed
                  by a small sigmoidal neural net can also be computed by a
                  small network of spiking neurons. This article does not assume
                  prior knowledge about spiking neurons, and it contains an
                  extensive list of references to the currently available
                  literature on computations in networks of spiking neurons and
                  relevant results from neurobiology."
}

@article {Merolla668,
  author          = {Merolla, Paul A. and Arthur, John V. and Alvarez-Icaza,
                  Rodrigo and Cassidy, Andrew S. and Sawada, Jun and Akopyan,
                  Filipp and Jackson, Bryan L. and Imam, Nabil and Guo, Chen and
                  Nakamura, Yutaka and Brezzo, Bernard and Vo, Ivan and Esser,
                  Steven K. and Appuswamy, Rathinakumar and Taba, Brian and
                  Amir, Arnon and Flickner, Myron D. and Risk, William P. and
                  Manohar, Rajit and Modha, Dharmendra S.},
  title           = {A million spiking-neuron integrated circuit with a scalable
                  communication network and interface},
  volume          = 345,
  number          = 6197,
  pages           = {668--673},
  year            = 2014,
  doi             = {10.1126/science.1254642},
  publisher       = {American Association for the Advancement of Science},
  abstract        = {Computers are nowhere near as versatile as our own brains.
                  Merolla et al. applied our present knowledge of the structure
                  and function of the brain to design a new computer chip that
                  uses the same wiring rules and architecture. The flexible,
                  scalable chip operated efficiently in real time, while using
                  very little power.Science, this issue p. 668 Inspired by the
                  brain{\textquoteright}s structure, we have developed an
                  efficient, scalable, and flexible non{\textendash}von Neumann
                  architecture that leverages contemporary silicon technology.
                  To demonstrate, we built a 5.4-billion-transistor chip with
                  4096 neurosynaptic cores interconnected via an intrachip
                  network that integrates 1 million programmable spiking neurons
                  and 256 million configurable synapses. Chips can be tiled in
                  two dimensions via an interchip communication interface,
                  seamlessly scaling the architecture to a cortexlike sheet of
                  arbitrary size. The architecture is well suited to many
                  applications that use complex neural networks in real time,
                  for example, multiobject detection and classification. With
                  400-pixel-by-240-pixel video input at 30 frames per second,
                  the chip consumes 63 milliwatts.},
  issn            = {0036-8075},
  URL             = {https://science.sciencemag.org/content/345/6197/668},
  eprint          =
                  {https://science.sciencemag.org/content/345/6197/668.full.pdf},
  journal         = {Science}
}

@inproceedings{Mitrokhin2019,
  Author          = {Mitrokhin, Anton and Ye, Chengxi and Fermuller, Cornelia
                  and Aloimonos, Yiannis and Delbruck, Tobi},
  Booktitle       = {2019 IEEE/RSJ International Conference on Intelligent
                  Robots and Systems (IROS)},
  Date-Added      = {2020-01-30 16:05:14 +0800},
  Date-Modified   = {2020-01-30 16:05:31 +0800},
  Title           = {{EV-IMO: Motion Segmentation Dataset and Learning Pipeline
                  for Event Cameras}},
  Year            = 2019
}

@incollection{NIPS2018_7415,
  title           = {SLAYER: Spike Layer Error Reassignment in Time},
  author          = {Shrestha, Sumit Bam and Orchard, Garrick},
  booktitle       = {Advances in Neural Information Processing Systems 31},
  editor          = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman
                  and N. Cesa-Bianchi and R. Garnett},
  pages           = {1412--1421},
  year            = 2018,
  publisher       = {Curran Associates, Inc.},
  url             =
                  {http://papers.nips.cc/paper/7415-slayer-spike-layer-error-reassignment-in-time.pdf}
}

@incollection{NIPS2018_7417,
  title           = {Gradient Descent for Spiking Neural Networks},
  author          = {Huh, Dongsung and Sejnowski, Terrence J},
  booktitle       = {Advances in Neural Information Processing Systems 31},
  editor          = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman
                  and N. Cesa-Bianchi and R. Garnett},
  pages           = {1433--1443},
  year            = 2018,
  publisher       = {Curran Associates, Inc.},
  url             =
                  {http://papers.nips.cc/paper/7417-gradient-descent-for-spiking-neural-networks.pdf}
}

@incollection{NIPS2018_7892,
  title           = {Neural Ordinary Differential Equations},
  author          = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt,
                  Jesse and Duvenaud, David K},
  booktitle       = {Advances in Neural Information Processing Systems 31},
  editor          = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman
                  and N. Cesa-Bianchi and R. Garnett},
  pages           = {6571--6583},
  year            = 2018,
  publisher       = {Curran Associates, Inc.},
  url             =
                  {http://papers.nips.cc/paper/7892-neural-ordinary-differential-equations.pdf}
}

@article{Rathi2018,
  Author          = {N. {Rathi} and K. {Roy}},
  Date-Added      = {2020-01-31 13:41:17 +0800},
  Date-Modified   = {2020-01-31 13:41:30 +0800},
  Doi             = {10.1109/TETCI.2018.2872014},
  Issn            = {2471-285X},
  Journal         = {IEEE Transactions on Emerging Topics in Computational
                  Intelligence},
  Keywords        = {Synapses;Biological neural networks;Timing;Hidden Markov
                  models;Unsupervised learning;Multimodal Learning;Spiking
                  Neural Network;Spike Timing Dependent Plasticity;Unsupervised
                  Learning;Cross-modal Connections;Synergistic Learning},
  Pages           = {1-11},
  Title           = {STDP-Based Unsupervised Multimodal Learning With
                  Cross-Modal Processing in Spiking Neural Network},
  Year            = 2018,
  Bdsk-Url-1      = {https://doi.org/10.1109/TETCI.2018.2872014}
}

@article{SHRESTHA201733,
  title           = "Robust spike-train learning in spike-event based weight
                  update",
  journal         = "Neural Networks",
  volume          = 96,
  pages           = "33 - 46",
  year            = 2017,
  issn            = "0893-6080",
  doi             = "https://doi.org/10.1016/j.neunet.2017.08.010",
  url             =
                  "http://www.sciencedirect.com/science/article/pii/S0893608017302009",
  author          = "Sumit Bam Shrestha and Qing Song",
  keywords        = "Spiking neural network, Multilayer spike-train learning,
                  Supervised learning, Weight convergence, Robust stability,
                  Adaptive learning rate",
  abstract        = "Supervised learning algorithms in a spiking neural network
                  either learn a spike-train pattern for a single neuron
                  receiving input spike-train from multiple input synapses or
                  learn to output the first spike time in a feedforward network
                  setting. In this paper, we build upon spike-event based weight
                  update strategy to learn continuous spike-train in a spiking
                  neural network with a hidden layer using a dead zone on–off
                  based adaptive learning rate rule which ensures convergence of
                  the learning process in the sense of weight convergence and
                  robustness of the learning process to external disturbances.
                  Based on different benchmark problems, we compare this new
                  method with other relevant spike-train learning algorithms.
                  The results show that the speed of learning is much improved
                  and the rate of successful learning is also greatly improved."
}

@article{Severa2016SpikingNA,
  title           = "Spiking network algorithms for scientific computing",
  author          = "William Severa and Ojas Parekh and Kristofor D. Carlson and
                  Conrad D. James and James B. Aimone",
  journal         = "2016 IEEE International Conference on Rebooting Computing
                  (ICRC)",
  year            = 2016,
  pages           = "1-8"
}

@article{SnnSlam,
  author          = "Tang, Guangzhi and Shah, Arpit and Michmizos, Konstantinos
                  P.",
  title           = "Spiking Neural Network on Neuromorphic Hardware for
                  Energy-Efficient Unidimensional Slam",
  journal         = "CoRR",
  year            = 2019,
  url             = "http://arxiv.org/abs/1903.02504v2",
  abstract        = "Energy-efficient simultaneous localization and mapping
                  (SLAM) is crucial for mobile robots exploring unknown
                  environments. The mammalian brain solves SLAM via a network of
                  specialized neurons, exhibiting asynchronous computations and
                  event-based communications, with very low energy consumption.
                  We propose a brain-inspired spiking neural network (SNN)
                  architecture that solves the unidimensional SLAM by
                  introducing spike-based reference frame transformation, visual
                  likelihood computation, and Bayesian inference. We integrated
                  our neuromorphic algorithm to Intel's Loihi neuromorphic
                  processor, a non-Von Neumann hardware that mimics the brain's
                  computing paradigms. We performed comparative analyses for
                  accuracy and energy-efficiency between our neuromorphic
                  approach and the GMapping algorithm, which is widely used in
                  small environments. Our Loihi-based SNN architecture consumes
                  100 times less energy than GMapping run on a CPU while having
                  comparable accuracy in head direction localization and
                  map-generation. These results pave the way for scaling our
                  approach towards active-SLAM alternative solutions for
                  Loihi-controlled autonomous robots.",
  archivePrefix   = "arXiv",
  eprint          = "1903.02504",
  primaryClass    = "cs.RO",
}

@article{TAVANAEI201947,
  title           = "Deep learning in spiking neural networks",
  journal         = "Neural Networks",
  volume          = 111,
  pages           = "47 - 63",
  year            = 2019,
  issn            = "0893-6080",
  doi             = "https://doi.org/10.1016/j.neunet.2018.12.002",
  url             =
                  "http://www.sciencedirect.com/science/article/pii/S0893608018303332",
  author          = "Amirhossein Tavanaei and Masoud Ghodrati and Saeed Reza
                  Kheradpisheh and Timothée Masquelier and Anthony Maida",
  keywords        = "Deep learning, Spiking neural network, Biological
                  plausibility, Machine learning, Power-efficient architecture",
  abstract        = "In recent years, deep learning has revolutionized the field
                  of machine learning, for computer vision in particular. In
                  this approach, a deep (multilayer) artificial neural network
                  (ANN) is trained, most often in a supervised manner using
                  backpropagation. Vast amounts of labeled training examples are
                  required, but the resulting classification accuracy is truly
                  impressive, sometimes outperforming humans. Neurons in an ANN
                  are characterized by a single, static, continuous-valued
                  activation. Yet biological neurons use discrete spikes to
                  compute and transmit information, and the spike times, in
                  addition to the spike rates, matter. Spiking neural networks
                  (SNNs) are thus more biologically realistic than ANNs, and are
                  arguably the only viable option if one wants to understand how
                  the brain computes at the neuronal description level. The
                  spikes of biological neurons are sparse in time and space, and
                  event-driven. Combined with bio-plausible local learning
                  rules, this makes it easier to build low-power, neuromorphic
                  hardware for SNNs. However, training deep SNNs remains a
                  challenge. Spiking neurons’ transfer function is usually
                  non-differentiable, which prevents using backpropagation. Here
                  we review recent supervised and unsupervised methods to train
                  deep SNNs, and compare them in terms of accuracy and
                  computational cost. The emerging picture is that SNNs still
                  lag behind ANNs in terms of accuracy, but the gap is
                  decreasing, and can even vanish on some tasks, while SNNs
                  typically require many fewer operations and are the better
                  candidates to process spatio-temporal data."
}

@article{VITANZA20153122,
  title           = "Spiking neural controllers in multi-agent competitive
                  systems for adaptive targeted motor learning",
  journal         = "Journal of the Franklin Institute",
  volume          = 352,
  number          = 8,
  pages           = "3122 - 3143",
  year            = 2015,
  note            = "Special Issue on Advances in Nonlinear Dynamics and
                  Control",
  issn            = "0016-0032",
  doi             = "https://doi.org/10.1016/j.jfranklin.2015.04.014",
  url             =
                  "http://www.sciencedirect.com/science/article/pii/S001600321500174X",
  author          = "Alessandra Vitanza and Luca Patané and Paolo Arena",
  abstract        = "The proposed work introduces a neural control strategy for
                  guiding adaptation in spiking neural structures acting as
                  nonlinear controllers in a group of bio-inspired robots which
                  compete in reaching targets in a virtual environment. The
                  neural structures embedded into each agent are inspired by a
                  specific part of the insect brain, namely Central Complex,
                  devoted to detect, learn and memorize visual features for
                  targeted motor control. A reduced-order model of a spiking
                  neuron is used as the basic building block for the neural
                  controller. The control methodology employs bio-inspired,
                  correlation based learning mechanisms like Spike timing
                  dependent plasticity with the addition of a
                  reward/punishment-based method experimentally found in
                  insects. The reference signal for the overall multi-agent
                  control system is imposed by a global reward, which guides
                  motor learning to direct each agent towards specific visual
                  targets. The neural controllers within the agents start from
                  identical conditions: the learning strategy induces each robot
                  to show anticipated targeting actions upon specific visual
                  stimuli. The whole control structure also contributes to make
                  the robots refractory or more sensitive to specific visual
                  stimuli, showing distinct preferences in future choices. This
                  leads to an environmentally induced, targeted motor control,
                  even without a direct communication among the agents, giving
                  robots, while running, the ability to perform adaptation in
                  real-time. Experiments, carried out in a dynamic simulation
                  environment, show the suitability of the proposed approach.
                  Specific performance indexes, like Shannon׳s Entropy, are
                  adopted to quantitatively analyze diversity and specialization
                  within the group."
}

@article{abbott99_lapic_introd_integ_and_fire,
  author          = {L.F Abbott},
  title           = {Lapicque's Introduction of the Integrate-And-Fire Model
                  Neuron (1907)},
  journal         = {Brain Research Bulletin},
  volume          = 50,
  number          = {5-6},
  pages           = {303-304},
  year            = 1999,
  doi             = {10.1016/s0361-9230(99)00161-6},
  url             = {https://doi.org/10.1016/s0361-9230(99)00161-6},
  DATE_ADDED      = {Sun Mar 22 12:28:53 2020},
}

@article{aenugu19_reinf_learn_with_spikin_coagen,
  author          = {Aenugu, Sneha and Sharma, Abhishek and Yelamarthi,
                  Sasikiran and Hazan, Hananel and Thomas, Philip S. and Kozma,
                  Robert},
  title           = {Reinforcement Learning With Spiking Coagents},
  journal         = {CoRR},
  year            = 2019,
  url             = {http://arxiv.org/abs/1910.06489v2},
  abstract        = {Neuroscientific theory suggests that dopaminergic neurons
                  broadcast global reward prediction errors to large areas of
                  the brain influencing the synaptic plasticity of the neurons
                  in those regions. We build on this theory to propose a
                  multi-agent learning framework with spiking neurons in the
                  generalized linear model (GLM) formulation as agents, to solve
                  reinforcement learning (RL) tasks. We show that a network of
                  GLM spiking agents connected in a hierarchical fashion, where
                  each spiking agent modulates its firing policy based on local
                  information and a global prediction error, can learn complex
                  action representations to solve RL tasks. We further show how
                  leveraging principles of modularity and population coding
                  inspired from the brain can help reduce variance in the
                  learning updates making it a viable optimization technique.},
  archivePrefix   = {arXiv},
  eprint          = {1910.06489},
  primaryClass    = {cs.LG},
}

@article{aiskinLee,
  author          = {Lee, Wang Wei and Tan, Yu Jun and Yao, Haicheng and Li, Si
                  and See, Hian Hian and Hon, Matthew and Ng, Kian Ann and
                  Xiong, Betty and Ho, John S. and Tee, Benjamin C. K.},
  title           = {A neuro-inspired artificial peripheral nervous system for
                  scalable electronic skins},
  volume          = 4,
  number          = 32,
  elocation-id    = {eaax2198},
  year            = 2019,
  doi             = {10.1126/scirobotics.aax2198},
  publisher       = {Science Robotics},
  abstract        = {The human sense of touch is essential for dexterous tool
                  usage, spatial awareness, and social communication. Equipping
                  intelligent human-like androids and prosthetics with
                  electronic skins{\textemdash}a large array of sensors
                  spatially distributed and capable of rapid somatosensory
                  perception{\textemdash}will enable them to work
                  collaboratively and naturally with humans to manipulate
                  objects in unstructured living environments. Previously
                  reported tactile-sensitive electronic skins largely transmit
                  the tactile information from sensors serially, resulting in
                  readout latency bottlenecks and complex wiring as the number
                  of sensors increases. Here, we introduce the Asynchronously
                  Coded Electronic Skin (ACES){\textemdash}a neuromimetic
                  architecture that enables simultaneous transmission of
                  thermotactile information while maintaining exceptionally low
                  readout latencies, even with array sizes beyond 10,000
                  sensors. We demonstrate prototype arrays of up to 240
                  artificial mechanoreceptors that transmitted events
                  asynchronously at a constant latency of 1 ms while maintaining
                  an ultra-high temporal precision of \&lt;60 ns, thus resolving
                  fine spatiotemporal features necessary for rapid tactile
                  perception. Our platform requires only a single electrical
                  conductor for signal propagation, realizing sensor arrays that
                  are dynamically reconfigurable and robust to damage. We
                  anticipate that the ACES platform can be integrated with a
                  wide range of skin-like sensors for artificial intelligence
                  (AI){\textendash}enhanced autonomous robots, neuroprosthetics,
                  and neuromorphic computing hardware for dexterous object
                  manipulation and somatosensory perception.},
  URL             = {https://robotics.sciencemag.org/content/4/32/eaax2198},
  eprint          =
                  {https://robotics.sciencemag.org/content/4/32/eaax2198.full.pdf},
  journal         = {Science Robotics}
}

@inproceedings{allenil_surfac,
  author          = {P. Allen},
  title           = {Surface descriptions from vision and touch},
  booktitle       = {Proceedings. 1984 IEEE International Conference on Robotics
                  and Automation},
  year            = {nil},
  pages           = {nil},
  doi             = {10.1109/robot.1984.1087191},
  url             = {https://doi.org/10.1109/robot.1984.1087191},
  DATE_ADDED      = {Wed Mar 25 13:11:27 2020},
  month           = {-},
}

@article{blouw18_bench_keywor_spott_effic_neurom_hardw,
  author          = {Blouw, Peter and Choo, Xuan and Hunsberger, Eric and
                  Eliasmith, Chris},
  title           = {Benchmarking Keyword Spotting Efficiency on Neuromorphic
                  Hardware},
  journal         = {CoRR},
  year            = 2018,
  url             = {http://arxiv.org/abs/1812.01739v2},
  abstract        = {Using Intel's Loihi neuromorphic research chip and ABR's
                  Nengo Deep Learning toolkit, we analyze the inference speed,
                  dynamic power consumption, and energy cost per inference of a
                  two-layer neural network keyword spotter trained to recognize
                  a single phrase. We perform comparative analyses of this
                  keyword spotter running on more conventional hardware devices
                  including a CPU, a GPU, Nvidia's Jetson TX1, and the Movidius
                  Neural Compute Stick. Our results indicate that for this
                  inference application, Loihi outperforms all of these
                  alternatives on an energy cost per inference basis while
                  maintaining equivalent inference accuracy. Furthermore, an
                  analysis of tradeoffs between network size, inference speed,
                  and energy cost indicates that Loihi's comparative advantage
                  over other low-power computing devices improves for larger
                  networks.},
  archivePrefix   = {arXiv},
  eprint          = {1812.01739},
  primaryClass    = {cs.LG},
}

@article{calandra2018more,
  title           = {More than a feeling: Learning to grasp and regrasp using
                  vision and touch},
  author          = {Calandra, Roberto and Owens, Andrew and Jayaraman, Dinesh
                  and Lin, Justin and Yuan, Wenzhen and Malik, Jitendra and
                  Adelson, Edward H and Levine, Sergey},
  journal         = {IEEE Robotics and Automation Letters},
  volume          = 3,
  number          = 4,
  pages           = {3300--3307},
  year            = 2018,
  publisher       = {IEEE}
}

@inproceedings{chevallier2005distributed,
  Author          = {Chevallier, Sylvain and Paugam-Moisy, H{\'e}l{\`e}ne and
                  Lema{\^\i}tre, Fran{\c{c}}ois},
  Booktitle       = {Parallel and Distributed Computing and Networks},
  Date-Added      = {2020-01-31 13:47:15 +0800},
  Date-Modified   = {2020-01-31 13:47:15 +0800},
  Pages           = {393--398},
  Title           = {Distributed Processing for Modelling Real-Time Multimodal
                  Perception in a Virtual Robot.},
  Year            = 2005
}

@article{comsa19_tempor_codin_spikin_neural_networ,
  author          = {Comsa, Iulia M. and Potempa, Krzysztof and Versari, Luca
                  and Fischbacher, Thomas and Gesmundo, Andrea and Alakuijala,
                  Jyrki},
  title           = {Temporal Coding in Spiking Neural Networks With Alpha
                  Synaptic Function},
  journal         = {CoRR},
  year            = 2019,
  url             = {http://arxiv.org/abs/1907.13223v2},
  abstract        = {The timing of individual neuronal spikes is essential for
                  biological brains to make fast responses to sensory stimuli.
                  However, conventional artificial neural networks lack the
                  intrinsic temporal coding ability present in biological
                  networks. We propose a spiking neural network model that
                  encodes information in the relative timing of individual
                  neuron spikes. In classification tasks, the output of the
                  network is indicated by the first neuron to spike in the
                  output layer. This temporal coding scheme allows the
                  supervised training of the network with backpropagation, using
                  locally exact derivatives of the postsynaptic spike times with
                  respect to presynaptic spike times. The network operates using
                  a biologically-plausible alpha synaptic transfer function.
                  Additionally, we use trainable synchronisation pulses that
                  provide bias, add flexibility during training and exploit the
                  decay part of the alpha function. We show that such networks
                  can be trained successfully on noisy Boolean logic tasks and
                  on the MNIST dataset encoded in time. The results show that
                  the spiking neural network outperforms comparable spiking
                  models on MNIST and achieves similar quality to fully
                  connected conventional networks with the same architecture. We
                  also find that the spiking network spontaneously discovers two
                  operating regimes, mirroring the accuracy-speed trade-off
                  observed in human decision-making: a slow regime, where a
                  decision is taken after all hidden neurons have spiked and the
                  accuracy is very high, and a fast regime, where a decision is
                  taken very fast but the accuracy is lower. These results
                  demonstrate the computational power of spiking networks with
                  biological characteristics that encode information in the
                  timing of individual neurons. By studying temporal coding in
                  spiking networks, we aim to create building blocks towards
                  energy-efficient and more complex biologically-inspired neural
                  architectures.},
  archivePrefix   = {arXiv},
  eprint          = {1907.13223},
  primaryClass    = {cs.NE},
}

@article{davies2018loihi,
  title           = {Loihi: A neuromorphic manycore processor with on-chip
                  learning},
  author          = {Davies, Mike and Srinivasa, Narayan and Lin, Tsung-Han and
                  Chinya, Gautham and Cao, Yongqiang and Choday, Sri Harsha and
                  Dimou, Georgios and Joshi, Prasad and Imam, Nabil and Jain,
                  Shweta and others},
  journal         = {IEEE Micro},
  volume          = 38,
  number          = 1,
  pages           = {82--99},
  year            = 2018,
  publisher       = {IEEE}
}

@article{drimus2014design,
  title           = {Design of a flexible tactile sensor for classification of
                  rigid and deformable objects},
  author          = {Drimus, Alin and Kootstra, Gert and Bilberg, Arne and
                  Kragic, Danica},
  journal         = {Robotics and Autonomous Systems},
  volume          = 62,
  number          = 1,
  pages           = {3--15},
  year            = 2014,
  publisher       = {Elsevier}
}

@book{drubach2000brain,
  title           = {The brain explained},
  author          = {Drubach, Daniel},
  year            = 2000,
  publisher       = {Prentice Hall}
}

@INPROCEEDINGS{dvsgesture,
  author          = {A. {Amir} and B. {Taba} and D. {Berg} and T. {Melano} and
                  J. {McKinstry} and C. D. {Nolfo} and T. {Nayak} and A.
                  {Andreopoulos} and G. {Garreau} and M. {Mendoza} and J.
                  {Kusnitz} and M. {Debole} and S. {Esser} and T. {Delbruck} and
                  M. {Flickner} and D. {Modha}},
  booktitle       = {2017 IEEE Conference on Computer Vision and Pattern
                  Recognition (CVPR)},
  title           = {A Low Power, Fully Event-Based Gesture Recognition System},
  year            = 2017,
  pages           = {7388-7397},
  keywords        = {cameras;computer vision;gesture recognition;image
                  sensors;low-power electronics;neural nets;asynchronous data
                  representation;cameras;live DVS event stream;TrueNorth
                  chip;TrueNorth neurosynaptic processor;Dynamic Vision
                  Sensor;biologically inspired DVS;fixed frame rate;sparse data
                  representation;hand gesture recognition;DVS dataset;low power
                  fully event-based gesture recognition system;natively
                  event-based processor;spiking neurons;convolutional neural
                  network;CNN;illumination conditions;synchronous
                  processors;time 105.0 ms;power 200.0
                  mW;Cameras;Neurons;Gesture recognition;Voltage
                  control;Real-time systems;Sensors;Feature extraction},
  doi             = {10.1109/CVPR.2017.781},
  ISSN            = {1063-6919},
  month           = {July},
}

@article{fitzhugh1955mathematical,
  title           = {Mathematical models of threshold phenomena in the nerve
                  membrane},
  author          = {FitzHugh, Richard},
  journal         = {The bulletin of mathematical biophysics},
  volume          = 17,
  number          = 4,
  pages           = {257--278},
  year            = 1955,
  publisher       = {Springer}
}

@article{florian07_reinf_learn_throug_modul_spike,
  author          = {Răzvan V. Florian},
  title           = {Reinforcement Learning Through Modulation of
                  Spike-Timing-Dependent Synaptic Plasticity},
  journal         = {Neural Computation},
  volume          = 19,
  number          = 6,
  pages           = {1468-1502},
  year            = 2007,
  doi             = {10.1162/neco.2007.19.6.1468},
  url             = {https://doi.org/10.1162/neco.2007.19.6.1468},
  DATE_ADDED      = {Mon Nov 4 15:30:29 2019},
}

@inproceedings{florian2005,
  author          = {Florian, Răzvan},
  year            = 2005,
  month           = 10,
  pages           = {8 pp.-},
  title           = {A reinforcement learning algorithm for spiking neural
                  networks},
  volume          = 2005,
  isbn            = {0-7695-2453-2},
  journal         = {Proceedings - Seventh International Symposium on Symbolic
                  and Numeric Algorithms for Scientific Computing, SYNASC 2005},
  doi             = {10.1109/SYNASC.2005.13}
}

@inproceedings{gao2016deep,
  title           = {Deep learning for tactile understanding from visual and
                  haptic data},
  author          = {Gao, Yang and Hendricks, Lisa Anne and Kuchenbecker,
                  Katherine J and Darrell, Trevor},
  booktitle       = {2016 IEEE International Conference on Robotics and
                  Automation (ICRA)},
  pages           = {536--543},
  year            = 2016,
  organization    = {IEEE}
}

@article{gerstner2001framework,
  title           = {A framework for spiking neuron models-the spike response
                  model},
  author          = {Gerstner, Wulfram},
  journal         = {Handbook of Biological Physics},
  volume          = 4,
  number          = {BOOK\_CHAP},
  pages           = {469--516},
  year            = 2001,
  publisher       = {Elsevier}
}

@article{grossberg1987competitive,
  title           = {Competitive learning: From interactive activation to
                  adaptive resonance},
  author          = {Grossberg, Stephen},
  journal         = {Cognitive science},
  volume          = 11,
  number          = 1,
  pages           = {23--63},
  year            = 1987,
  publisher       = {Elsevier}
}

@article{guetig14_to_spike_or_when_to_spike,
  author          = {Robert G{\"u}tig},
  title           = {To Spike, Or When To Spike?},
  journal         = {Current Opinion in Neurobiology},
  volume          = 25,
  number          = {nil},
  pages           = {134-139},
  year            = 2014,
  doi             = {10.1016/j.conb.2014.01.004},
  url             = {https://doi.org/10.1016/j.conb.2014.01.004},
  DATE_ADDED      = {Fri Nov 1 16:23:04 2019},
}

@book{hebb1949organization,
  title           = {The organization of behavior: a neuropsychological theory},
  author          = {Hebb, Donald Olding},
  year            = 1949,
  publisher       = {J. Wiley; Chapman \& Hall}
}

@article{heeger2000poisson,
  title           = {Poisson model of spike generation},
  author          = {Heeger, David},
  journal         = {Handout, University of Standford},
  volume          = 5,
  pages           = {1--13},
  year            = 2000
}

@article{hochreiter1997long,
  title           = {Long short-term memory},
  author          = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal         = {Neural computation},
  volume          = 9,
  number          = 8,
  pages           = {1735--1780},
  year            = 1997,
  publisher       = {MIT Press}
}

@article{hodgkin1952quantitative,
  title           = {A quantitative description of membrane current and its
                  application to conduction and excitation in nerve},
  author          = {Hodgkin, Alan L and Huxley, Andrew F},
  journal         = {The Journal of physiology},
  volume          = 117,
  number          = 4,
  pages           = {500--544},
  year            = 1952,
  publisher       = {Wiley Online Library}
}

@article{ivanov19_moder_deep_reinf_learn_algor,
  author          = {Ivanov, Sergey and D'yakonov, Alexander},
  title           = {Modern Deep Reinforcement Learning Algorithms},
  journal         = {CoRR},
  year            = 2019,
  url             = {http://arxiv.org/abs/1906.10025v2},
  abstract        = {Recent advances in Reinforcement Learning, grounded on
                  combining classical theoretical results with Deep Learning
                  paradigm, led to breakthroughs in many artificial intelligence
                  tasks and gave birth to Deep Reinforcement Learning (DRL) as a
                  field of research. In this work latest DRL algorithms are
                  reviewed with a focus on their theoretical justification,
                  practical limitations and observed empirical properties.},
  archivePrefix   = {arXiv},
  eprint          = {1906.10025},
  primaryClass    = {cs.LG},
}

@article{izhikevich2003simple,
  title           = {Simple model of spiking neurons},
  author          = {Izhikevich, Eugene M},
  journal         = {IEEE Transactions on neural networks},
  volume          = 14,
  number          = 6,
  pages           = {1569--1572},
  year            = 2003,
  publisher       = {IEEE}
}

@article{izhikevich2004model,
  title           = {Which model to use for cortical spiking neurons?},
  author          = {Izhikevich, Eugene M},
  journal         = {IEEE transactions on neural networks},
  volume          = 15,
  number          = 5,
  pages           = {1063--1070},
  year            = 2004,
  publisher       = {Ieee}
}

@article{jolivet04_gener_integ_and_fire_model,
  author          = {Renaud Jolivet and Timothy J. Lewis and Wulfram Gerstner},
  title           = {Generalized Integrate-And-Fire Models of Neuronal Activity
                  Approximate Spike Trains of a Detailed Model To a High Degree
                  of Accuracy},
  journal         = {Journal of Neurophysiology},
  volume          = 92,
  number          = 2,
  pages           = {959-976},
  year            = 2004,
  doi             = {10.1152/jn.00190.2004},
  url             = {https://doi.org/10.1152/jn.00190.2004},
  DATE_ADDED      = {Sun Mar 22 18:02:35 2020},
}

@article{kaiser18_synap_plast_dynam_deep_contin,
  author          = {Kaiser, Jacques and Mostafa, Hesham and Neftci, Emre},
  title           = {Synaptic Plasticity Dynamics for Deep Continuous Local
                  Learning (DECOLLE)},
  journal         = {CoRR},
  year            = 2018,
  url             = {http://arxiv.org/abs/1811.10766v3},
  abstract        = {A growing body of work underlines striking similarities
                  between biological neural networks and recurrent, binary
                  neural networks. A relatively smaller body of work, however,
                  discusses similarities between learning dynamics employed in
                  deep artificial neural networks and synaptic plasticity in
                  spiking neural networks. The challenge preventing this is
                  largely caused by the discrepancy between the dynamical
                  properties of synaptic plasticity and the requirements for
                  gradient backpropagation. Learning algorithms that approximate
                  gradient backpropagation using locally synthesized gradients
                  can overcome this challenge. Here, we show that synthetic
                  gradients enable the derivation of Deep Continuous Local
                  Learning (DECOLLE) in spiking neural networks. DECOLLE is
                  capable of learning deep spatio-temporal representations from
                  spikes relying solely on local information. Synaptic
                  plasticity rules are derived systematically from user-defined
                  cost functions and neural dynamics by leveraging existing
                  autodifferentiation methods of machine learning frameworks. We
                  benchmark our approach on the MNIST and the event-based
                  neuromorphic DvsGesture dataset, on which DECOLLE performs
                  comparably to the state-of-the-art. DECOLLE networks provide
                  continuously learning machines that are relevant to biology
                  and supportive of event-based, low-power computer vision
                  architectures matching the accuracies of conventional
                  computers on tasks where temporal precision and speed are
                  essential.},
  archivePrefix   = {arXiv},
  eprint          = {1811.10766},
  primaryClass    = {cs.NE},
}

@InProceedings{ klaus_greff-proc-scipy-2017,
  author          = { {K}laus {G}reff and {A}aron {K}lein and {M}artin
                  {C}hovanec and {F}rank {H}utter and {J}\"urgen {S}chmidhuber },
  title           = { {T}he {S}acred {I}nfrastructure for {C}omputational
                  {R}esearch },
  booktitle       = { {P}roceedings of the 16th {P}ython in {S}cience
                  {C}onference },
  pages           = { 49 - 56 },
  year            = { 2017 },
  editor          = { {K}aty {H}uff and {D}avid {L}ippa and {D}illon {N}iederhut
                  and {M} {P}acer },
  doi             = { 10.25080/shinma-7f4c6e7-008 }
}

@inproceedings{lee2019making,
  title           = {Making sense of vision and touch: Self-supervised learning
                  of multimodal representations for contact-rich tasks},
  author          = {Lee, Michelle A and Zhu, Yuke and Srinivasan, Krishnan and
                  Shah, Parth and Savarese, Silvio and Fei-Fei, Li and Garg,
                  Animesh and Bohg, Jeannette},
  booktitle       = {2019 International Conference on Robotics and Automation
                  (ICRA)},
  pages           = {8943--8950},
  year            = 2019,
  organization    = {IEEE}
}

@inproceedings{lee2019making,
  title           = {Making sense of vision and touch: Self-supervised learning
                  of multimodal representations for contact-rich tasks},
  author          = {Lee, Michelle A and Zhu, Yuke and Srinivasan, Krishnan and
                  Shah, Parth and Savarese, Silvio and Fei-Fei, Li and Garg,
                  Animesh and Bohg, Jeannette},
  booktitle       = {2019 International Conference on Robotics and Automation
                  (ICRA)},
  pages           = {8943--8950},
  year            = 2019,
  organization    = {IEEE}
}

@article{legenstein08_learn_theor_rewar_modul_spike,
  author          = {Robert Legenstein and Dejan Pecevski and Wolfgang Maass},
  title           = {A Learning Theory for Reward-Modulated
                  Spike-Timing-Dependent Plasticity With Application To
                  Biofeedback},
  journal         = {PLoS Computational Biology},
  volume          = 4,
  number          = 10,
  pages           = {e1000180},
  year            = 2008,
  doi             = {10.1371/journal.pcbi.1000180},
  url             = {https://doi.org/10.1371/journal.pcbi.1000180},
  DATE_ADDED      = {Sun Mar 22 23:10:21 2020},
}

@inproceedings{levine2017handeye,
  author          = {Levine, Sergey and Pastor, Peter and Krizhevsky, Alex and
                  Quillen, Deirdre},
  year            = 2017,
  month           = 03,
  pages           = {173-184},
  title           = {Learning Hand-Eye Coordination for Robotic Grasping with
                  Large-Scale Data Collection},
  isbn            = {978-3-319-50114-7},
  doi             = {10.1007/978-3-319-50115-4_16}
}

@article{li18_deep_reinf_learn,
  author          = {Li, Yuxi},
  title           = {Deep Reinforcement Learning},
  journal         = {CoRR},
  year            = 2018,
  url             = {http://arxiv.org/abs/1810.06339v1},
  abstract        = {We discuss deep reinforcement learning in an overview
                  style. We draw a big picture, filled with details. We discuss
                  six core elements, six important mechanisms, and twelve
                  applications, focusing on contemporary work, and in historical
                  contexts. We start with background of artificial intelligence,
                  machine learning, deep learning, and reinforcement learning
                  (RL), with resources. Next we discuss RL core elements,
                  including value function, policy, reward, model, exploration
                  vs. exploitation, and representation. Then we discuss
                  important mechanisms for RL, including attention and memory,
                  unsupervised learning, hierarchical RL, multi-agent RL,
                  relational RL, and learning to learn. After that, we discuss
                  RL applications, including games, robotics, natural language
                  processing (NLP), computer vision, finance, business
                  management, healthcare, education, energy, transportation,
                  computer systems, and, science, engineering, and art. Finally
                  we summarize briefly, discuss challenges and opportunities,
                  and close with an epilogue.},
  archivePrefix   = {arXiv},
  eprint          = {1810.06339},
  primaryClass    = {cs.LG},
}

@inproceedings{li18_slip_detec_combin_tactil_visual_infor,
  author          = {Jianhua Li and Siyuan Dong and Edward Adelson},
  title           = {Slip Detection with Combined Tactile and Visual
                  Information},
  booktitle       = {2018 IEEE International Conference on Robotics and
                  Automation (ICRA)},
  year            = 2018,
  pages           = {nil},
  doi             = {10.1109/icra.2018.8460495},
  url             = {https://doi.org/10.1109/icra.2018.8460495},
  DATE_ADDED      = {Wed Mar 25 14:14:55 2020},
  month           = 5,
}

@INPROCEEDINGS{li2016energyefficiency,
  author          = {D. {Li} and X. {Chen} and M. {Becchi} and Z. {Zong}},
  booktitle       = {2016 IEEE International Conferences on Big Data and Cloud
                  Computing (BDCloud), Social Computing and Networking
                  (SocialCom), Sustainable Computing and Communications
                  (SustainCom) (BDCloud-SocialCom-SustainCom)},
  title           = {Evaluating the Energy Efficiency of Deep Convolutional
                  Neural Networks on CPUs and GPUs},
  year            = 2016,
  pages           = {477-484},
  keywords        = {energy conservation;graphics processing units;learning
                  (artificial intelligence);neural nets;energy efficiency;deep
                  convolutional neural networks;CPU;central processing
                  unit;GPU;graphics processing unit;deep learning
                  framework;Graphics processing units;Biological neural
                  networks;Training;Energy efficiency;Machine learning;Energy
                  consumption;Hardware;energy-efficiency;neural networks;deep
                  learning;GPUs},
  doi             = {10.1109/BDCloud-SocialCom-SustainCom.2016.76},
  ISSN            = {null},
  month           = {Oct},
}

@inproceedings{lin2019learning,
  title           = {Learning to identify object instances by touch: Tactile
                  recognition via multimodal matching},
  author          = {Lin, Justin and Calandra, Roberto and Levine, Sergey},
  booktitle       = {2019 International Conference on Robotics and Automation
                  (ICRA)},
  pages           = {3644--3650},
  year            = 2019,
  organization    = {IEEE}
}

@book{liu18_robot_tactil_percep_under,
  author          = {Huaping Liu and Fuchun Sun},
  title           = {Robotic Tactile Perception and Understanding},
  year            = 2018,
  publisher       = {Springer Singapore},
  url             = {https://doi.org/10.1007/978-981-10-6171-4},
  DATE_ADDED      = {Wed Mar 25 15:06:12 2020},
  doi             = {10.1007/978-981-10-6171-4},
  pages           = {nil},
  series          = {[]},
}

@article{liu2016visual,
  title           = {Visual--tactile fusion for object recognition},
  author          = {Liu, Huaping and Yu, Yuanlong and Sun, Fuchun and Gu,
                  Jason},
  journal         = {IEEE Transactions on Automation Science and Engineering},
  volume          = 14,
  number          = 2,
  pages           = {996--1008},
  year            = 2016,
  publisher       = {IEEE}
}

@inproceedings{mansouri2019speech,
  Author          = {Mansouri-Benssassi, Esma and Ye, Juan},
  Booktitle       = {2019 International Joint Conference on Neural Networks
                  (IJCNN)},
  Date-Added      = {2020-01-31 13:47:56 +0800},
  Date-Modified   = {2020-01-31 13:47:56 +0800},
  Organization    = {IEEE},
  Pages           = {1--8},
  Title           = {Speech Emotion Recognition With Early Visual Cross-modal
                  Enhancement Using Spiking Neural Networks},
  Year            = 2019
}

@inproceedings{maqueda2018event,
  Author          = {Maqueda, Ana I and Loquercio, Antonio and Gallego,
                  Guillermo and Garc$\backslash$'$\backslash$ia, Narciso and
                  Scaramuzza, Davide},
  Booktitle       = {Proceedings of the IEEE Conference on Computer Vision and
                  Pattern Recognition},
  Date-Added      = {2020-01-30 16:06:07 +0800},
  Date-Modified   = {2020-01-30 16:06:07 +0800},
  Pages           = {5419--5427},
  Title           = {{Event-based vision meets deep learning on steering
                  prediction for self-driving cars}},
  Year            = 2018
}

@inproceedings{meier2016tactile,
  title           = {Tactile convolutional networks for online slip and rotation
                  detection},
  author          = {Meier, Martin and Patzelt, Florian and Haschke, Robert and
                  Ritter, Helge J},
  booktitle       = {International Conference on Artificial Neural Networks},
  pages           = {12--19},
  year            = 2016,
  organization    = {Springer}
}

@article{morris81_voltag_oscil_barnac_giant_muscl_fiber,
  author          = {C. Morris and H. Lecar},
  title           = {Voltage Oscillations in the Barnacle Giant Muscle Fiber},
  journal         = {Biophysical Journal},
  volume          = 35,
  number          = 1,
  pages           = {193-213},
  year            = 1981,
  doi             = {10.1016/s0006-3495(81)84782-0},
  url             = {https://doi.org/10.1016/s0006-3495(81)84782-0},
  DATE_ADDED      = {Sat Mar 21 17:35:05 2020},
}

@article{neftci19_surrog_gradien_learn_spikin_neural_networ,
  author          = {Neftci, Emre O. and Mostafa, Hesham and Zenke, Friedemann},
  title           = {Surrogate Gradient Learning in Spiking Neural Networks},
  journal         = {CoRR},
  year            = 2019,
  url             = {http://arxiv.org/abs/1901.09948v2},
  archivePrefix   = {arXiv},
  eprint          = {1901.09948},
  primaryClass    = {cs.NE},
}

@misc{openai_gym,
  author          = {OpenAI},
  howpublished    = {https://gym.openai.com/envs/CartPole-v0/},
  note            = {Online; accessed 02 November 2019},
  title           = {OpenAI Gym},
  year            = 2019,
}

@article{orchard15_conver_static_image_datas_to,
  author          = {Garrick Orchard and Ajinkya Jayawant and Gregory K. Cohen
                  and Nitish Thakor},
  title           = {Converting Static Image Datasets To Spiking Neuromorphic
                  Datasets Using Saccades},
  journal         = {Frontiers in Neuroscience},
  volume          = 9,
  number          = {nil},
  pages           = {nil},
  year            = 2015,
  doi             = {10.3389/fnins.2015.00437},
  url             = {https://doi.org/10.3389/fnins.2015.00437},
  DATE_ADDED      = {Mon Mar 23 02:37:53 2020},
}

@inproceedings{pavlidisil_spikin,
  author          = {N.G. Pavlidis and O.K. Tasoulis and V.P. Plagianakos and G.
                  Nikiforidis and M.N. Vrahatis},
  title           = {Spiking neural network training using evolutionary
                  algorithms},
  booktitle       = {Proceedings. 2005 IEEE International Joint Conference on
                  Neural Networks, 2005.},
  year            = {nil},
  pages           = {nil},
  doi             = {10.1109/ijcnn.2005.1556240},
  url             = {https://doi.org/10.1109/ijcnn.2005.1556240},
  DATE_ADDED      = {Mon Mar 23 00:20:31 2020},
  month           = {-},
}

@article{pfeiffer2018deep,
  title           = {Deep learning with spiking neurons: opportunities and
                  challenges},
  author          = {Pfeiffer, Michael and Pfeil, Thomas},
  journal         = {Frontiers in neuroscience},
  volume          = 12,
  year            = 2018,
  publisher       = {Frontiers Media SA}
}

@article{pfister06_optim_spike_timin_depen_plast,
  author          = {Jean-Pascal Pfister and Taro Toyoizumi and David Barber and
                  Wulfram Gerstner},
  title           = {Optimal Spike-Timing-Dependent Plasticity for Precise
                  Action Potential Firing in Supervised Learning},
  journal         = {Neural Computation},
  volume          = 18,
  number          = 6,
  pages           = {1318-1348},
  year            = 2006,
  doi             = {10.1162/neco.2006.18.6.1318},
  url             = {https://doi.org/10.1162/neco.2006.18.6.1318},
  DATE_ADDED      = {Mon Mar 23 01:38:10 2020},
}

@InProceedings{pmlr-v89-o-connor19a,
  title           = {Training a Spiking Neural Network with Equilibrium
                  Propagation},
  author          = {O'Connor, Peter and Gavves, Efstratios and Welling, Max},
  booktitle       = {Proceedings of Machine Learning Research},
  pages           = {1516--1523},
  year            = 2019,
  editor          = {Chaudhuri, Kamalika and Sugiyama, Masashi},
  volume          = 89,
  series          = {Proceedings of Machine Learning Research},
  month           = {16--18 Apr},
  publisher       = {PMLR},
  pdf             =
                  {http://proceedings.mlr.press/v89/o-connor19a/o-connor19a.pdf},
  url             = {http://proceedings.mlr.press/v89/o-connor19a.html},
  abstract        = {Backpropagation is almost universally used to train
                  artificial neural networks. However, there are several reasons
                  that backpropagation could not be plausibly implemented by
                  biological neurons. Among these are the facts that (1)
                  biological neurons appear to lack any mechanism for sending
                  gradients backwards across synapses, and (2) biological
                  “spiking” neurons emit binary signals, whereas
                  back-propagation requires that neurons communicate continuous
                  values between one another. Recently, Scellier and Bengio
                  [2017], demonstrated an alternative to backpropagation, called
                  Equilibrium Propagation, wherein gradients are implicitly
                  computed by the dynamics of the neural network, so that
                  neurons do not need an internal mechanism for backpropagation
                  of gradients. This provides an interesting solution to problem
                  (1). In this paper, we address problem (2) by proposing a way
                  in which Equilibrium Propagation can be implemented with
                  neurons which are constrained to just communicate binary
                  values at each time step. We show that with appropriate
                  step-size annealing, we can converge to the same fixed-point
                  as a real-valued neural network, and that with predictive
                  coding, we can make this convergence much faster. We
                  demonstrate that the resulting model can be used to train a
                  spiking neural network using the update scheme from
                  Equilibrium propagation.}
}

@inproceedings{reinecke2014experimental,
  title           = {Experimental comparison of slip detection strategies by
                  tactile sensing with the BioTac{\textregistered} on the DLR
                  hand arm system},
  author          = {Reinecke, Jens and Dietrich, Alexander and Schmidt, Florian
                  and Chalon, Maxime},
  booktitle       = {2014 IEEE international Conference on Robotics and
                  Automation (ICRA)},
  pages           = {2742--2748},
  year            = 2014,
  organization    = {IEEE}
}

@article{rueckauer16_theor_tools_conver_analog_to,
  author          = {Rueckauer, Bodo and Lungu, Iulia-Alexandra and Hu, Yuhuang
                  and Pfeiffer, Michael},
  title           = {Theory and Tools for the Conversion of Analog To Spiking
                  Convolutional Neural Networks},
  journal         = {CoRR},
  year            = 2016,
  url             = {http://arxiv.org/abs/1612.04052v1},
  abstract        = {Deep convolutional neural networks (CNNs) have shown great
                  potential for numerous real-world machine learning
                  applications, but performing inference in large CNNs in
                  real-time remains a challenge. We have previously demonstrated
                  that traditional CNNs can be converted into deep spiking
                  neural networks (SNNs), which exhibit similar accuracy while
                  reducing both latency and computational load as a consequence
                  of their data-driven, event-based style of computing. Here we
                  provide a novel theory that explains why this conversion is
                  successful, and derive from it several new tools to convert a
                  larger and more powerful class of deep networks into SNNs. We
                  identify the main sources of approximation errors in previous
                  conversion methods, and propose simple mechanisms to fix these
                  issues. Furthermore, we develop spiking implementations of
                  common CNN operations such as max-pooling, softmax, and
                  batch-normalization, which allow almost loss-less conversion
                  of arbitrary CNN architectures into the spiking domain.
                  Empirical evaluation of different network architectures on the
                  MNIST and CIFAR10 benchmarks leads to the best SNN results
                  reported to date.},
  archivePrefix   = {arXiv},
  eprint          = {1612.04052},
  primaryClass    = {stat.ML},
}

@article{sboev18_spikin_neural_networ_reinf_learn,
  author          = {Alexander Sboev and Danila Vlasov and Roman Rybka and
                  Alexey Serenko},
  title           = {Spiking Neural Network Reinforcement Learning Method Based
                  on Temporal Coding and Stdp},
  journal         = {Procedia Computer Science},
  volume          = 145,
  number          = {nil},
  pages           = {458-463},
  year            = 2018,
  doi             = {10.1016/j.procs.2018.11.107},
  url             = {https://doi.org/10.1016/j.procs.2018.11.107},
  DATE_ADDED      = {Mon Sep 30 11:08:34 2019},
}

@article{shan2017robotic,
  title           = {Robotic tactile perception of object properties: A review},
  author          = {Luo, Shan and Bimbo, Joao and Dahiya, Ravinder and Liu,
                  Hongbin},
  journal         = {Mechatronics},
  volume          = 48,
  pages           = {54--67},
  year            = 2017,
  publisher       = {Elsevier}
}

@inproceedings{sinapov14_learn,
  author          = {Jivko Sinapov and Connor Schenck and Alexander Stoytchev},
  title           = {Learning relational object categories using behavioral
                  exploration and multimodal perception},
  booktitle       = {2014 IEEE International Conference on Robotics and
                  Automation (ICRA)},
  year            = 2014,
  pages           = {nil},
  doi             = {10.1109/icra.2014.6907696},
  url             = {https://doi.org/10.1109/icra.2014.6907696},
  DATE_ADDED      = {Wed Mar 25 13:18:10 2020},
  month           = 5,
}

@inproceedings{spikeprop,
  author          = {Bohte, Sander and Kok, Joost and Poutré, Johannes},
  year            = 2000,
  month           = 01,
  pages           = {419-424},
  title           = {SpikeProp: backpropagation for networks of spiking
                  neurons.},
  volume          = 48,
  journal         = {ESANN}
}

@article{stemmler1996single,
  title           = {A single spike suffices: the simplest form of stochastic
                  resonance in model neurons},
  author          = {Stemmler, Martin},
  journal         = {Network: Computation in Neural Systems},
  volume          = 7,
  number          = 4,
  pages           = {687--716},
  year            = 1996,
  publisher       = {Taylor \& Francis}
}

@article{stemmler96_singl_spike_suffic,
  author          = {Martin Stemmler},
  title           = {A Single Spike Suffices: the Simplest Form of Stochastic
                  Resonance in Model Neurons},
  journal         = {Network: Computation in Neural Systems},
  volume          = 7,
  number          = 4,
  pages           = {687-716},
  year            = 1996,
  doi             = {10.1088/0954-898x_7_4_005},
  url             = {https://doi.org/10.1088/0954-898x_7_4_005},
  DATE_ADDED      = {Sat Nov 2 19:32:20 2019},
}

@inproceedings{su2015force,
  title           = {Force estimation and slip detection/classification for grip
                  control using a biomimetic tactile sensor},
  author          = {Su, Zhe and Hausman, Karol and Chebotar, Yevgen and
                  Molchanov, Artem and Loeb, Gerald E and Sukhatme, Gaurav S and
                  Schaal, Stefan},
  booktitle       = {2015 IEEE-RAS 15th International Conference on Humanoid
                  Robots (Humanoids)},
  pages           = {297--303},
  year            = 2015,
  organization    = {IEEE}
}

@article{such17_deep_neuroev,
  author          = {Such, Felipe Petroski and Madhavan, Vashisht and Conti,
                  Edoardo and Lehman, Joel and Stanley, Kenneth O. and Clune,
                  Jeff},
  title           = {Deep Neuroevolution: Genetic Algorithms Are a Competitive
                  Alternative for Training Deep Neural Networks for
                  Reinforcement Learning},
  journal         = {CoRR},
  year            = 2017,
  url             = {http://arxiv.org/abs/1712.06567v3},
  abstract        = {Deep artificial neural networks (DNNs) are typically
                  trained via gradient-based learning algorithms, namely
                  backpropagation. Evolution strategies (ES) can rival
                  backprop-based algorithms such as Q-learning and policy
                  gradients on challenging deep reinforcement learning (RL)
                  problems. However, ES can be considered a gradient-based
                  algorithm because it performs stochastic gradient descent via
                  an operation similar to a finite-difference approximation of
                  the gradient. That raises the question of whether
                  non-gradient-based evolutionary algorithms can work at DNN
                  scales. Here we demonstrate they can: we evolve the weights of
                  a DNN with a simple, gradient-free, population-based genetic
                  algorithm (GA) and it performs well on hard deep RL problems,
                  including Atari and humanoid locomotion. The Deep GA
                  successfully evolves networks with over four million free
                  parameters, the largest neural networks ever evolved with a
                  traditional evolutionary algorithm. These results (1) expand
                  our sense of the scale at which GAs can operate, (2) suggest
                  intriguingly that in some cases following the gradient is not
                  the best choice for optimizing performance, and (3) make
                  immediately available the multitude of neuroevolution
                  techniques that improve performance. We demonstrate the latter
                  by showing that combining DNNs with novelty search, which
                  encourages exploration on tasks with deceptive or sparse
                  reward functions, can solve a high-dimensional problem on
                  which reward-maximizing algorithms (e.g.\ DQN, A3C, ES, and
                  the GA) fail. Additionally, the Deep GA is faster than ES,
                  A3C, and DQN (it can train Atari in
                  ${\raise.17ex\hbox{$\scriptstyle\sim$}}$4 hours on one desktop
                  or ${\raise.17ex\hbox{$\scriptstyle\sim$}}$1 hour distributed
                  on 720 cores), and enables a state-of-the-art, up to
                  10,000-fold compact encoding technique.},
  archivePrefix   = {arXiv},
  eprint          = {1712.06567},
  primaryClass    = {cs.NE},
}

@online{tds_artif_biolog_neural_networ,
  author          = {Richard Nagyfi},
  title           = {The differences between Artificial and Biological Neural
                  Networks},
  url             =
                  {https://towardsdatascience.com/the-differences-between-artificial-and-biological-neural-networks-a8b46db828b7},
  urldate         = {Online; accessed 21 March 2020},
  year            = 2018,
}

@article{teka14_neuron_spike_timin_adapt_descr,
  author          = {Wondimu Teka and Toma M. Marinov and Fidel Santamaria},
  title           = {Neuronal Spike Timing Adaptation Described With a
                  Fractional Leaky Integrate-And-Fire Model},
  journal         = {PLoS Computational Biology},
  volume          = 10,
  number          = 3,
  pages           = {e1003526},
  year            = 2014,
  doi             = {10.1371/journal.pcbi.1003526},
  url             = {https://doi.org/10.1371/journal.pcbi.1003526},
  DATE_ADDED      = {Sun Mar 22 12:30:30 2020},
}

@article{thorpe2001spike,
  title           = {Spike-based strategies for rapid processing},
  author          = {Thorpe, Simon and Delorme, Arnaud and Van Rullen, Rufin},
  journal         = {Neural networks},
  volume          = 14,
  number          = {6-7},
  pages           = {715--725},
  year            = 2001,
  publisher       = {Elsevier}
}

@article{training_deep_snn_bpp_lee,
  author          = {Lee, Jun and Delbruck, Tobi and Pfeiffer, Michael},
  year            = 2016,
  month           = 08,
  title           = {Training Deep Spiking Neural Networks Using
                  Backpropagation},
  volume          = 10,
  journal         = {Frontiers in Neuroscience},
  doi             = {10.3389/fnins.2016.00508}
}

@article{urbanczik09_gradien_learn_rule_tempot,
  author          = {Robert Urbanczik and Walter Senn},
  title           = {A Gradient Learning Rule for the Tempotron},
  journal         = {Neural Computation},
  volume          = 21,
  number          = 2,
  pages           = {340-352},
  year            = 2009,
  doi             = {10.1162/neco.2008.09-07-605},
  url             = {https://doi.org/10.1162/neco.2008.09-07-605},
  DATE_ADDED      = {Fri Nov 1 16:00:33 2019},
}

@techreport{Gallego2018,
Archiveprefix = {arXiv},
Arxivid = {arXiv:1904.08405v1},
Author = {Gallego, Guillermo and Delbr, Tobi and Orchard, Garrick and Bartolozzi, Chiara and Taba, Brian and Censi, Andrea and Daniilidis, Kostas and Scaramuzza, Davide and Leutenegger, Stefan and Davison, Andrew},
Date-Added = {2020-01-30 15:56:57 +0800},
Date-Modified = {2020-01-30 15:56:57 +0800},
Eprint = {arXiv:1904.08405v1},
Pages = {1--25},
Title = {{Event-based Vision : A Survey}},
Year = {2018}
}

@inproceedings{varley2017visual,
  title           = {Visual-tactile geometric reasoning},
  author          = {Varley, Jacob and Watkins, David and Allen, Peter},
  booktitle       = {RSS Workshop},
  year            = 2017
}

@article{victor05_spike_train_metric,
  author          = {Jonathan D Victor},
  title           = {Spike Train Metrics},
  journal         = {Current Opinion in Neurobiology},
  volume          = 15,
  number          = 5,
  pages           = {585-592},
  year            = 2005,
  doi             = {10.1016/j.conb.2005.08.002},
  url             = {https://doi.org/10.1016/j.conb.2005.08.002},
  DATE_ADDED      = {Mon Mar 23 21:36:50 2020},
}

@article{ycb2015,
  Author          = {B. {Calli} and A. {Walsman} and A. {Singh} and S.
                  {Srinivasa} and P. {Abbeel} and A. M. {Dollar}},
  Doi             = {10.1109/MRA.2015.2448951},
  Issn            = {1558-223X},
  Journal         = {IEEE Robotics Automation Magazine},
  Keywords        = {benchmark testing;learning (artificial
                  intelligence);planning (artificial
                  intelligence);robots;benchmarking;Yale-CMU-Berkeley object
                  set;Yale-CMU-Berkeley model set;robotic manipulation
                  research;manipulation tests;associated
                  database;high-resolution red green blue plus depth scans;RGB-D
                  scans;physical properties;geometric models;planning software
                  platforms;learning;mechanical design;Benchmark
                  testing;Robots;Data models;Object detection;Solid
                  modeling;Databases;Prosthetics},
  Month           = {Sep.},
  Number          = 3,
  Pages           = {36-52},
  Title           = {Benchmarking in Manipulation Research: Using the
                  Yale-CMU-Berkeley Object and Model Set},
  Volume          = 22,
  Year            = 2015,
  Bdsk-Url-1      = {https://doi.org/10.1109/MRA.2015.2448951}
}

@article{yuan2017gelsight,
  title           = {Gelsight: High-resolution robot tactile sensors for
                  estimating geometry and force},
  author          = {Yuan, Wenzhen and Dong, Siyuan and Adelson, Edward H},
  journal         = {Sensors},
  volume          = 17,
  number          = 12,
  pages           = 2762,
  year            = 2017,
  publisher       = {Multidisciplinary Digital Publishing Institute}
}

@article{zambrano16_fast_effic_async_neural_comput,
  author          = {Zambrano, Davide and Bohte, Sander M.},
  title           = {Fast and Efficient Asynchronous Neural Computation With
                  Adapting Spiking Neural Networks},
  journal         = {CoRR},
  year            = 2016,
  url             = {http://arxiv.org/abs/1609.02053v1},
  abstract        = {Biological neurons communicate with a sparing exchange of
                  pulses - spikes. It is an open question how real spiking
                  neurons produce the kind of powerful neural computation that
                  is possible with deep artificial neural networks, using only
                  so very few spikes to communicate. Building on recent insights
                  in neuroscience, we present an Adapting Spiking Neural Network
                  (ASNN) based on adaptive spiking neurons. These spiking
                  neurons efficiently encode information in spike-trains using a
                  form of Asynchronous Pulsed Sigma-Delta coding while
                  homeostatically optimizing their firing rate. In the proposed
                  paradigm of spiking neuron computation, neural adaptation is
                  tightly coupled to synaptic plasticity, to ensure that
                  downstream neurons can correctly decode upstream spiking
                  neurons. We show that this type of network is inherently able
                  to carry out asynchronous and event-driven neural computation,
                  while performing identical to corresponding artificial neural
                  networks (ANNs). In particular, we show that these adaptive
                  spiking neurons can be drop in replacements for ReLU neurons
                  in standard feedforward ANNs comprised of such units. We
                  demonstrate that this can also be successfully applied to a
                  ReLU based deep convolutional neural network for classifying
                  the MNIST dataset. The ASNN thus outperforms current Spiking
                  Neural Networks (SNNs) implementations, while responding (up
                  to) an order of magnitude faster and using an order of
                  magnitude fewer spikes. Additionally, in a streaming setting
                  where frames are continuously classified, we show that the
                  ASNN requires substantially fewer network updates as compared
                  to the corresponding ANN.},
  archivePrefix   = {arXiv},
  eprint          = {1609.02053v1},
  primaryClass    = {cs.NE},
}

@article{zenke17_super,
  author          = {Zenke, Friedemann and Ganguli, Surya},
  title           = {Superspike: Supervised Learning in Multi-Layer Spiking
                  Neural Networks},
  journal         = {CoRR},
  year            = 2017,
  url             = {http://arxiv.org/abs/1705.11146v2},
  abstract        = {A vast majority of computation in the brain is performed by
                  spiking neural networks. Despite the ubiquity of such spiking,
                  we currently lack an understanding of how biological spiking
                  neural circuits learn and compute in-vivo, as well as how we
                  can instantiate such capabilities in artificial spiking
                  circuits in-silico. Here we revisit the problem of supervised
                  learning in temporally coding multi-layer spiking neural
                  networks. First, by using a surrogate gradient approach, we
                  derive SuperSpike, a nonlinear voltage-based three factor
                  learning rule capable of training multi-layer networks of
                  deterministic integrate-and-fire neurons to perform nonlinear
                  computations on spatiotemporal spike patterns. Second,
                  inspired by recent results on feedback alignment, we compare
                  the performance of our learning rule under different credit
                  assignment strategies for propagating output errors to hidden
                  units. Specifically, we test uniform, symmetric and random
                  feedback, finding that simpler tasks can be solved with any
                  type of feedback, while more complex tasks require symmetric
                  feedback. In summary, our results open the door to obtaining a
                  better scientific understanding of learning and computation in
                  spiking neural networks by advancing our ability to train them
                  to solve nonlinear problems involving transformations between
                  different spatiotemporal spike-time patterns.},
  archivePrefix   = {arXiv},
  eprint          = {1705.11146},
  primaryClass    = {q-bio.NC},
}

@inproceedings{zhu2018ev,
  Author          = {Zhu, Alex Zihao and Yuan, Liangzhe},
  Booktitle       = {Robotics: Science and Systems},
  Date-Added      = {2020-01-30 16:00:27 +0800},
  Date-Modified   = {2020-01-30 16:00:27 +0800},
  Title           = {{EV-FlowNet: Self-Supervised Optical Flow Estimation for
                  Event-based Cameras}},
  Year            = 2018
}
